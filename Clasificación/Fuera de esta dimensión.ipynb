{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuera de esta dimensión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este trabajo, entraré a la competencia de Kaggle llamada *Spaceship Titanic* (https://www.kaggle.com/competitions/spaceship-titanic) para predecir qué individuos han sido expulsados a otra dimensión tras un accidente espacial utilizando un modelo de clasificación. Para ello, entrenaré modelos de regresión logística, LDA, DTs, RFs, y Adaboost, y tras escoger uno, realizaré predicciones y las enviaré al sitio de Kaggle para que evaluen la precisión de las predicciones. \n",
    "\n",
    "A lo largo de este trabajo se encontrarán referencias in-code acerca de uso de IA en el mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos las variables:\n",
    "\n",
    "- ``PassengerId``: *A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always*.\n",
    "    - no se si es categorica o cuantitiva. es único tho\n",
    "- ``HomePlanet``: *The planet the passenger departed from, typically their planet of permanent residence.*\n",
    "    - categorica: categoria\n",
    "- ``CryoSleep``: *Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.*\n",
    "    - categorica: booleana\n",
    "- ``Cabin``: *The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.*\n",
    "    - categorica: categoria\n",
    "- ``Destination``: *The planet the passenger will be debarking to.*\n",
    "    - categorica: categoria\n",
    "- ``Age``: *The age of the passenger.*\n",
    "    - cuantitiva\n",
    "- ``VIP``: *Whether the passenger has paid for special VIP service during the voyage.*\n",
    "    - categorica: booleana\n",
    "- ``RoomService, FoodCourt, ShoppingMall, Spa, VRDeck``: *Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.*\n",
    "    - cuanatitva\n",
    "- ``Name``: *The first and last names of the passenger.*\n",
    "    - categorica: categoria\n",
    "- ``Transported``: *Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.*\n",
    "    - categorica: booleana\n",
    "    - TARGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploracion de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiero analizar:\n",
    "- tamaño de datos\n",
    "- tipo de datos\n",
    "- huecos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinemos primero el tamaño de los datos. Espero ~8700 observaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño dfTrain: (8693, 14)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dfTrain = pd.read_csv(\"train.csv\")\n",
    "dfTest = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Tamaño dfTrain: {dfTrain.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ahora su tipo de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### huecos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos si hay valores NaN en alguna columna:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay son muchos y no quiero arriesgarme a borrar 2000 observaciones (asumiendo que son cada una distintas como worst case) ni tampoco rellenar por rellenar, analizaré las variables de la base de datos y usaré una metodología para rellenar huecos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores NaN por columna en datos x: PassengerId       0\n",
      "HomePlanet      201\n",
      "CryoSleep       217\n",
      "Cabin           199\n",
      "Destination     182\n",
      "Age             179\n",
      "VIP             203\n",
      "RoomService     181\n",
      "FoodCourt       183\n",
      "ShoppingMall    208\n",
      "Spa             183\n",
      "VRDeck          188\n",
      "Name            200\n",
      "Transported       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Valores NaN por columna en datos x: {dfTrain.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valores NaN por columna: PassengerId     0\n",
      "HomePlanet      0\n",
      "CryoSleep       0\n",
      "Cabin           0\n",
      "Destination     0\n",
      "Age             0\n",
      "VIP             0\n",
      "RoomService     0\n",
      "FoodCourt       0\n",
      "ShoppingMall    0\n",
      "Spa             0\n",
      "VRDeck          0\n",
      "Name            0\n",
      "Transported     0\n",
      "dtype: int64\n",
      "\n",
      "Tamaño dfTrain: (8693, 14)\n",
      "\n",
      "Tipo de datos dfTrain: PassengerId      object\n",
      "HomePlanet       object\n",
      "CryoSleep        object\n",
      "Cabin            object\n",
      "Destination      object\n",
      "Age             float64\n",
      "VIP              object\n",
      "RoomService     float64\n",
      "FoodCourt       float64\n",
      "ShoppingMall    float64\n",
      "Spa             float64\n",
      "VRDeck          float64\n",
      "Name             object\n",
      "Transported        bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ESTAS INSTRUCCIONES LE DI A GITHUB COPILOT AI:\n",
    "\n",
    "ocupo que programes en camelCase y en español\n",
    "\n",
    "pon esta linea de codigo: nombresUnicos = dfTrain.Name.value_counts()[dfTrain.Name.value_counts() == 1].index \n",
    "\n",
    "Luego, necesito rellenar huecos en una base de datos 'dfTrain'. Hay 14 variables. Para las siguientes variables quiero que tomes las siguients acciones:\n",
    "\n",
    "1. [RoomService, FoodCourt, ShoppingMail, Spa, VRDeck]: Si hay hueco en alguna de estas 5, quiero que tomes la moda de las otras 4 de la misma observación. Si no hay moda, toma el promedio de las otras 4.\n",
    "2. [HomePlanet, Destination, Cabin, VIP]: toma la moda de las 3 observaciones anteriores y 3 posteriores a la actual.\n",
    "3. Name: \n",
    "    si hay una moda (es decir, si algun valor se repite) en los 3 cabins antes y despues de ese:\n",
    "        - obtener primer nombre de uno de los únicos\n",
    "        - obtener segundo nombre de alguno de los de la media\n",
    "        - agregar nombre completo en observacion actual, conformado por el primer nombre y segundo nombre obtenidos\n",
    "    si no:\n",
    "        agarra nombre completo uno de los que aparecen ndms una vez al azar y ponlo también en la observación actual\n",
    "4. Cryosleep: \n",
    "    si hay puros ceros en alguna de las variables [RoomService, FoodCourt, ShoppingMail, Spa, VRDeck]:\n",
    "        pon este valor como True.\n",
    "    Si no:\n",
    "        ponle que False \n",
    "5. Age: pon el promedio de las tres observaciones anteriores y los tres siguientes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "# Obtener nombres únicos\n",
    "nombresUnicos = dfTrain.Name.value_counts()[dfTrain.Name.value_counts() == 1].index\n",
    "\n",
    "# 1. Rellenar [RoomService, FoodCourt, ShoppingMail, Spa, VRDeck]\n",
    "servicios = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "for index, row in dfTrain.iterrows():\n",
    "    for servicio in servicios:\n",
    "        if pd.isna(row[servicio]):\n",
    "            otrosServicios = [row[s] for s in servicios if s != servicio and not pd.isna(row[s])]\n",
    "            if len(otrosServicios) > 0:\n",
    "                moda = pd.Series(otrosServicios).mode()\n",
    "                if len(moda) > 0:\n",
    "                    dfTrain.at[index, servicio] = moda[0]\n",
    "                else:\n",
    "                    dfTrain.at[index, servicio] = np.mean(otrosServicios)\n",
    "\n",
    "# 2. Rellenar [HomePlanet, Destination, Cabin, VIP]\n",
    "variablesCategoricas = ['HomePlanet', 'Destination', 'Cabin', 'VIP']\n",
    "for index in range(len(dfTrain)):\n",
    "    for variable in variablesCategoricas:\n",
    "        if pd.isna(dfTrain.at[index, variable]):\n",
    "            inicio = max(0, index - 3)\n",
    "            fin = min(len(dfTrain), index + 4)\n",
    "            moda = dfTrain[variable][inicio:fin].mode()\n",
    "            if len(moda) > 0:\n",
    "                dfTrain.at[index, variable] = moda[0]             \n",
    "                \n",
    "# 3. Rellenar Name\n",
    "for index in range(len(dfTrain)):\n",
    "    if pd.isna(dfTrain.at[index, 'Name']):\n",
    "        inicio = max(0, index - 3)\n",
    "        fin = min(len(dfTrain), index + 4)\n",
    "        modaCabin = dfTrain['Cabin'][inicio:fin].mode()\n",
    "        if len(modaCabin) > 0:\n",
    "            primerNombre = np.random.choice(nombresUnicos).split()[0]\n",
    "            segundoNombre = np.random.choice(dfTrain['Name'][inicio:fin].dropna().values).split()[1]\n",
    "            dfTrain.at[index, 'Name'] = f\"{primerNombre} {segundoNombre}\"\n",
    "        else:\n",
    "            dfTrain.at[index, 'Name'] = np.random.choice(nombresUnicos)  \n",
    "\n",
    "# 4. Rellenar Cryosleep\n",
    "for index, row in dfTrain.iterrows():\n",
    "    if pd.isna(row['CryoSleep']):\n",
    "        if all(row[servicio] == 0 for servicio in servicios):\n",
    "            dfTrain.at[index, 'CryoSleep'] = True\n",
    "        else:\n",
    "            dfTrain.at[index, 'CryoSleep'] = False     \n",
    "\n",
    "# 5. Rellenar Age\n",
    "for index in range(len(dfTrain)):\n",
    "    if pd.isna(dfTrain.at[index, 'Age']):\n",
    "        inicio = max(0, index - 3)\n",
    "        fin = min(len(dfTrain), index + 4)\n",
    "        dfTrain.at[index, 'Age'] = dfTrain['Age'][inicio:fin].mean()\n",
    "\n",
    "print(f\"\\nValores NaN por columna: {dfTrain.isna().sum()}\")\n",
    "print(f\"\\nTamaño dfTrain: {dfTrain.shape}\")\n",
    "print(f\"\\nTipo de datos dfTrain: {dfTrain.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, **se eliminaron todos los huecos y se mantuvo igual el tamaño de los datos**, por lo que vemos que no hubo una eliminación indeseada de datos. Claro que la discusión de si la metodología para hacerlo es razonable, o en todo caso, la mejor, es un tema aparte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras haber eliminado los huecosHay variables con demasiada cardinalidad: ``PassengerId``, ``Cabin`` y ``Name``. Las eliminaré del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain = dfTrain.drop([\"PassengerId\", \"Cabin\", \"Name\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "habran modelos que trabajen mejor tras haber sido tratados con OHE, y otros que no. Por eso crearé dos dataFrames distintos. Uno para cada grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age  RoomService  FoodCourt  ShoppingMall     Spa  VRDeck  \\\n",
      "0  39.0          0.0        0.0           0.0     0.0     0.0   \n",
      "1  24.0        109.0        9.0          25.0   549.0    44.0   \n",
      "2  58.0         43.0     3576.0           0.0  6715.0    49.0   \n",
      "3  33.0          0.0     1283.0         371.0  3329.0   193.0   \n",
      "4  16.0        303.0       70.0         151.0   565.0     2.0   \n",
      "\n",
      "   HomePlanet_Europa  HomePlanet_Mars  CryoSleep_True  \\\n",
      "0                  1                0               0   \n",
      "1                  0                0               0   \n",
      "2                  1                0               0   \n",
      "3                  1                0               0   \n",
      "4                  0                0               0   \n",
      "\n",
      "   Destination_PSO J318.5-22  Destination_TRAPPIST-1e  VIP_True  \\\n",
      "0                          0                        1         0   \n",
      "1                          0                        1         0   \n",
      "2                          0                        1         1   \n",
      "3                          0                        1         0   \n",
      "4                          0                        1         0   \n",
      "\n",
      "   Transported_True  \n",
      "0                 0  \n",
      "1                 1  \n",
      "2                 0  \n",
      "3                 0  \n",
      "4                 1  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"GITHUB COPILOT\"\"\"\n",
    "\n",
    "oheDFTrain = dfTrain\n",
    "nombresDatosCategoricos = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Transported\"]\n",
    "indicesDatosCategoricos = oheDFTrain.columns.get_indexer(nombresDatosCategoricos)\n",
    "\n",
    "# trabajado con IA\n",
    "for i in indicesDatosCategoricos:\n",
    "    original_column_name = oheDFTrain.columns[i]\n",
    "    dummy = pd.get_dummies(oheDFTrain.iloc[:,i], prefix=original_column_name, drop_first=True, dtype=np.int64)\n",
    "    oheDFTrain = pd.concat([oheDFTrain,dummy],axis=1)\n",
    "\n",
    "# normal\n",
    "oheDFTrain = oheDFTrain.drop(nombresDatosCategoricos, axis=1)\n",
    "\n",
    "print(oheDFTrain.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age                          float64\n",
      "RoomService                  float64\n",
      "FoodCourt                    float64\n",
      "ShoppingMall                 float64\n",
      "Spa                          float64\n",
      "VRDeck                       float64\n",
      "HomePlanet_Europa              int64\n",
      "HomePlanet_Mars                int64\n",
      "CryoSleep_True                 int64\n",
      "Destination_PSO J318.5-22      int64\n",
      "Destination_TRAPPIST-1e        int64\n",
      "VIP_True                       int64\n",
      "Transported_True               int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(oheDFTrain.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escalemos los datos para que el modelo pueda funcionar adecuadamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x = oheDFTrain[oheDFTrain.columns[oheDFTrain.columns != \"Transported_True\"]]\n",
    "y = oheDFTrain[\"Transported_True\"]\n",
    "\n",
    "# AI\n",
    "scaler = StandardScaler()\n",
    "xEscalado = scaler.fit_transform(x)\n",
    "xEscalado = pd.DataFrame(xEscalado, columns=x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haré una regresión logística *multiple*, para predecir 0 o 1 a partir de las variables de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76253081 0.79211175 0.79457683 0.79293344 0.79046836]\n"
     ]
    }
   ],
   "source": [
    "# Tras leer docus de KFold y ayuda de IA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xEscalado, y, test_size=0.3, random_state=27, stratify=y) \n",
    "rlModelo = LogisticRegression()\n",
    "rlModelo.fit(xTrain, yTrain)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "rlCVScores = cross_val_score(rlModelo, xTrain, yTrain, cv=kf)\n",
    "print(rlCVScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de validación cruzada: 0.7865\n",
      "Desviación estándar de validación cruzada: 0.0121\n",
      "Exactitud en el conjunto de prueba: 0.7791\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.78      1295\n",
      "           1       0.78      0.79      0.78      1313\n",
      "\n",
      "    accuracy                           0.78      2608\n",
      "   macro avg       0.78      0.78      0.78      2608\n",
      "weighted avg       0.78      0.78      0.78      2608\n",
      "\n",
      "Matriz de confusión:\n",
      "[[1001  294]\n",
      " [ 282 1031]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"AI \"\"\"\n",
    "# Calcular el promedio y la desviación estándar\n",
    "promedioCV = rlCVScores.mean()\n",
    "desviacionCV = rlCVScores.std()\n",
    "\n",
    "print(f\"Promedio de validación cruzada: {promedioCV:.4f}\")\n",
    "print(f\"Desviación estándar de validación cruzada: {desviacionCV:.4f}\")\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "accuracyTest = rlModelo.score(xTest, yTest)\n",
    "print(f\"Exactitud en el conjunto de prueba: {accuracyTest:.4f}\")\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predicciones en el conjunto de prueba\n",
    "yPred = rlModelo.predict(xTest)\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(yTest, yPred))\n",
    "\n",
    "# Matriz de confusión\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(yTest, yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:       Transported_True   No. Observations:                 6085\n",
      "Model:                            GLM   Df Residuals:                     6073\n",
      "Model Family:                Binomial   Df Model:                           11\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -2728.2\n",
      "Date:                Mon, 31 Mar 2025   Deviance:                       5456.4\n",
      "Time:                        15:21:28   Pearson chi2:                 5.79e+03\n",
      "No. Iterations:                     7   Pseudo R-squ. (CS):             0.3871\n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "Age                          -0.1593      0.034     -4.649      0.000      -0.226      -0.092\n",
      "RoomService                  -0.7943      0.061    -13.047      0.000      -0.914      -0.675\n",
      "FoodCourt                     0.7510      0.073     10.344      0.000       0.609       0.893\n",
      "ShoppingMall                  0.3131      0.049      6.344      0.000       0.216       0.410\n",
      "Spa                          -1.8080      0.109    -16.534      0.000      -2.022      -1.594\n",
      "VRDeck                       -1.5985      0.107    -14.987      0.000      -1.808      -1.389\n",
      "HomePlanet_Europa             0.9417      0.061     15.464      0.000       0.822       1.061\n",
      "HomePlanet_Mars               0.2809      0.037      7.684      0.000       0.209       0.353\n",
      "CryoSleep_True                0.7224      0.040     18.056      0.000       0.644       0.801\n",
      "Destination_PSO J318.5-22    -0.1211      0.039     -3.133      0.002      -0.197      -0.045\n",
      "Destination_TRAPPIST-1e      -0.2138      0.043     -5.016      0.000      -0.297      -0.130\n",
      "VIP_True                     -0.0813      0.040     -2.027      0.043      -0.160      -0.003\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.api import GLM\n",
    "from statsmodels.genmod.families import Binomial\n",
    "\n",
    "resultados = GLM(yTrain, xTrain, family=Binomial()).fit()\n",
    "print(resultados.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor P para significancia general del modelo: 0.0\n"
     ]
    }
   ],
   "source": [
    "### HECHO CON GITHUB COPILOT -- Test para evaluar relevancia del modelo\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# Get the null deviance and model deviance\n",
    "null_deviance = resultados.null_deviance\n",
    "model_deviance = resultados.deviance\n",
    "\n",
    "# Degrees of freedom\n",
    "df_diff = resultados.df_model\n",
    "\n",
    "# Calculate the p-value\n",
    "p_value = chi2.sf(null_deviance - model_deviance, df_diff)\n",
    "print(f\"Valor P para significancia general del modelo: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las variables tienen aportaciones significativas a la variable de salida. Por tanto, habrá que hacer un LDA con todas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76581758 0.76992605 0.76581758 0.76170912 0.76910435]\n"
     ]
    }
   ],
   "source": [
    "### HECHO CON COPILOT\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Entrena el modelo de LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(xTrain, yTrain)\n",
    "\n",
    "ldaCVScores = cross_val_score(lda, xTrain, yTrain, cv=kf)\n",
    "print(ldaCVScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de validación cruzada: 0.7665\n",
      "Desviación estándar de validación cruzada: 0.0029\n",
      "Exactitud en el conjunto de prueba: 0.7565\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.77      1295\n",
      "           1       0.79      0.71      0.75      1313\n",
      "\n",
      "    accuracy                           0.76      2608\n",
      "   macro avg       0.76      0.76      0.76      2608\n",
      "weighted avg       0.76      0.76      0.76      2608\n",
      "\n",
      "Matriz de confusión:\n",
      "[[1045  250]\n",
      " [ 385  928]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"AI \"\"\"\n",
    "# Calcular el promedio y la desviación estándar\n",
    "promedioCV = ldaCVScores.mean()\n",
    "desviacionCV = ldaCVScores.std()\n",
    "\n",
    "print(f\"Promedio de validación cruzada: {promedioCV:.4f}\")\n",
    "print(f\"Desviación estándar de validación cruzada: {desviacionCV:.4f}\")\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "accuracyTest = lda.score(xTest, yTest)\n",
    "print(f\"Exactitud en el conjunto de prueba: {accuracyTest:.4f}\")\n",
    "\n",
    "# Predicciones en el conjunto de prueba\n",
    "yPred = lda.predict(xTest)\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(yTest, yPred))\n",
    "\n",
    "# Matriz de confusión\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(yTest, yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA vs Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es visible como el promedio de puntiación en CV es un poco mejor en la regresión logística que en LDA, aunque la regresión tiene mayor sensibilidad clase 1 que para clase 0, lo contrario a LDA.\n",
    "\n",
    "además, en prueba de accuracy es un poco mejor en la regresión que en LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenemos un arbol de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha optimo: 0.001762825843151294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "tree = DTC().fit(xTrain, yTrain)\n",
    "\n",
    "# se tienen muchas observaciones, entonces con pocos folds se puede generalizar bien para datos fuera de entrenamiento\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "### HECHO CON COPILOT\n",
    "# Obtén el camino de poda basado en complejidad\n",
    "path = tree.cost_complexity_pruning_path(xTrain, yTrain)\n",
    "\n",
    "# Extrae los valores de ccp_alpha\n",
    "ccp_alphas = path.ccp_alphas  # Valores de alpha relevantes\n",
    "impurities = path.impurities  # Impurezas asociadas\n",
    "\n",
    "cv_scores = []\n",
    "\n",
    "ccp = np.linspace(0.001, ccp_alphas.max(), 150) \n",
    "for alpha in ccp:\n",
    "    prunedTree = DTC(ccp_alpha=alpha)\n",
    "    cv_scores.append(np.mean(cross_val_score(prunedTree, xTrain, yTrain, cv=skf, scoring='f1')))\n",
    "alpha = ccp[np.argmax(cv_scores)]\n",
    "print(f\"Alpha optimo: {alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80115037 0.7748562  0.75924404 0.79046836 0.80115037]\n"
     ]
    }
   ],
   "source": [
    "prunedTree = DTC(ccp_alpha=alpha).fit(xTrain, yTrain)\n",
    "dtcCVScores = cross_val_score(prunedTree, xTrain, yTrain, cv=kf)\n",
    "print(dtcCVScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de validación cruzada: 0.7854\n",
      "Desviación estándar de validación cruzada: 0.0162\n",
      "Exactitud en el conjunto de prueba: 0.7742\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77      1295\n",
      "           1       0.76      0.80      0.78      1313\n",
      "\n",
      "    accuracy                           0.77      2608\n",
      "   macro avg       0.77      0.77      0.77      2608\n",
      "weighted avg       0.77      0.77      0.77      2608\n",
      "\n",
      "Matriz de confusión:\n",
      "[[ 967  328]\n",
      " [ 261 1052]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"AI \"\"\"\n",
    "# Calcular el promedio y la desviación estándar\n",
    "promedioCV = dtcCVScores.mean()\n",
    "desviacionCV = dtcCVScores.std()\n",
    "\n",
    "print(f\"Promedio de validación cruzada: {promedioCV:.4f}\")\n",
    "print(f\"Desviación estándar de validación cruzada: {desviacionCV:.4f}\")\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "accuracyTest = prunedTree.score(xTest, yTest)\n",
    "print(f\"Exactitud en el conjunto de prueba: {accuracyTest:.4f}\")\n",
    "\n",
    "# Predicciones en el conjunto de prueba\n",
    "yPred = prunedTree.predict(xTest)\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(yTest, yPred))\n",
    "\n",
    "# Matriz de confusión\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(yTest, yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son resultados parecidos a ambas metodologías anteriores. En exactitud es un poco mejor que LDA, pero un poco peor que la regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora trabajremos con RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7863599  0.78471652 0.76335251 0.77403451 0.79046836]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Referencias de: https://www.youtube.com/watch?v=_QuGM_FW9eo\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(xTrain, yTrain)\n",
    "\n",
    "rfCVScores = cross_val_score(rf, xTrain, yTrain, cv=kf)\n",
    "print(rfCVScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de validación cruzada: 0.7798\n",
      "Desviación estándar de validación cruzada: 0.0098\n",
      "Exactitud en el conjunto de prueba: 0.7933\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.79      1295\n",
      "           1       0.80      0.79      0.79      1313\n",
      "\n",
      "    accuracy                           0.79      2608\n",
      "   macro avg       0.79      0.79      0.79      2608\n",
      "weighted avg       0.79      0.79      0.79      2608\n",
      "\n",
      "Matriz de confusión:\n",
      "[[1035  260]\n",
      " [ 279 1034]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"AI \"\"\"\n",
    "# Calcular el promedio y la desviación estándar\n",
    "promedioCV = rfCVScores.mean()\n",
    "desviacionCV = rfCVScores.std()\n",
    "\n",
    "print(f\"Promedio de validación cruzada: {promedioCV:.4f}\")\n",
    "print(f\"Desviación estándar de validación cruzada: {desviacionCV:.4f}\")\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "accuracyTest = rf.score(xTest, yTest)\n",
    "print(f\"Exactitud en el conjunto de prueba: {accuracyTest:.4f}\")\n",
    "\n",
    "# Predicciones en el conjunto de prueba\n",
    "yPred = rf.predict(xTest)\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(yTest, yPred))\n",
    "\n",
    "# Matriz de confusión\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(yTest, yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es interesante, porque se obtuvo un resultado un poco mejor en exactitud en RF que con regresion logistica. También lo es en promedio de CV, pero al considerar sus promedios y sus desviaciones estándar son similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentare con otro modelo. AdaBoost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78471652 0.7863599  0.77814297 0.79622021 0.78307313]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"referencia: https://www.youtube.com/watch?v=vlkhsbOATBo\n",
    "\n",
    "es de xgboost pero de ahi agarre la idea de solo hacer el fit y el cv\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ab = AdaBoostClassifier()\n",
    "ab.fit(xTrain, yTrain)\n",
    "\n",
    "abCVScores = cross_val_score(rf, xTrain, yTrain, cv=kf)\n",
    "print(abCVScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de validación cruzada: 0.7857\n",
      "Desviación estándar de validación cruzada: 0.0059\n",
      "Exactitud en el conjunto de prueba: 0.7500\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77      1295\n",
      "           1       0.80      0.67      0.73      1313\n",
      "\n",
      "    accuracy                           0.75      2608\n",
      "   macro avg       0.76      0.75      0.75      2608\n",
      "weighted avg       0.76      0.75      0.75      2608\n",
      "\n",
      "Matriz de confusión:\n",
      "[[1075  220]\n",
      " [ 432  881]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"AI \"\"\"\n",
    "# Calcular el promedio y la desviación estándar\n",
    "promedioCV = abCVScores.mean()\n",
    "desviacionCV = abCVScores.std()\n",
    "\n",
    "print(f\"Promedio de validación cruzada: {promedioCV:.4f}\")\n",
    "print(f\"Desviación estándar de validación cruzada: {desviacionCV:.4f}\")\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "accuracyTest = ab.score(xTest, yTest)\n",
    "print(f\"Exactitud en el conjunto de prueba: {accuracyTest:.4f}\")\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predicciones en el conjunto de prueba\n",
    "yPred = ab.predict(xTest)\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(yTest, yPred))\n",
    "\n",
    "# Matriz de confusión\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(yTest, yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay este modelo tuvo la peor performance en exactitud, pero probablemente hay otras cosas que se le pueden hacer para mejorarlo y no lo consideré."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICCIONES FINALES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### escala de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dado que transformé los datos para entrenar, necesito transformar los datos de entrada con los que voy a predecir, pero NO rellenaré huecos, sino tendría fuga de datos, creo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = dfTest.drop([\"PassengerId\", \"Cabin\", \"Name\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age  RoomService  FoodCourt  ShoppingMall     Spa  VRDeck  \\\n",
      "0  27.0          0.0        0.0           0.0     0.0     0.0   \n",
      "1  19.0          0.0        9.0           0.0  2823.0     0.0   \n",
      "2  31.0          0.0        0.0           0.0     0.0     0.0   \n",
      "3  38.0          0.0     6652.0           0.0   181.0   585.0   \n",
      "4  20.0         10.0        0.0         635.0     0.0     0.0   \n",
      "\n",
      "   HomePlanet_Europa  HomePlanet_Mars  CryoSleep_True  \\\n",
      "0                  0                0               1   \n",
      "1                  0                0               0   \n",
      "2                  1                0               1   \n",
      "3                  1                0               0   \n",
      "4                  0                0               0   \n",
      "\n",
      "   Destination_PSO J318.5-22  Destination_TRAPPIST-1e  VIP_True  \n",
      "0                          0                        1         0  \n",
      "1                          0                        1         0  \n",
      "2                          0                        0         0  \n",
      "3                          0                        1         0  \n",
      "4                          0                        1         0  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"GITHUB COPILOT\"\"\"\n",
    "\n",
    "oheDFTest = dfTest\n",
    "nombresDatosCategoricos = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\"]\n",
    "indicesDatosCategoricos = oheDFTest.columns.get_indexer(nombresDatosCategoricos)\n",
    "\n",
    "# trabajado con IA\n",
    "for i in indicesDatosCategoricos:\n",
    "    original_column_name = oheDFTest.columns[i]\n",
    "    dummy = pd.get_dummies(oheDFTest.iloc[:,i], prefix=original_column_name, drop_first=True, dtype=np.int64)\n",
    "    oheDFTest = pd.concat([oheDFTest,dummy],axis=1)\n",
    "\n",
    "# normal\n",
    "oheDFTest = oheDFTest.drop(nombresDatosCategoricos, axis=1)\n",
    "\n",
    "print(oheDFTest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTestReal = oheDFTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Age  RoomService  FoodCourt  ShoppingMall       Spa    VRDeck  \\\n",
      "0 -0.127954    -0.333158  -0.281070     -0.283630 -0.270688 -0.263009   \n",
      "1 -0.684676    -0.333158  -0.275430     -0.283630  2.237567 -0.263009   \n",
      "2  0.150407    -0.333158  -0.281070     -0.283630 -0.270688 -0.263009   \n",
      "3  0.637539    -0.333158   3.887668     -0.283630 -0.109868  0.252837   \n",
      "4 -0.615086    -0.318017  -0.281070      0.778304 -0.270688 -0.263009   \n",
      "\n",
      "   HomePlanet_Europa  HomePlanet_Mars  CryoSleep_True  \\\n",
      "0          -0.575358        -0.508685        1.331498   \n",
      "1          -0.575358        -0.508685       -0.751034   \n",
      "2           1.738049        -0.508685        1.331498   \n",
      "3           1.738049        -0.508685       -0.751034   \n",
      "4          -0.575358        -0.508685       -0.751034   \n",
      "\n",
      "   Destination_PSO J318.5-22  Destination_TRAPPIST-1e  VIP_True  \n",
      "0                  -0.318145                 0.657003 -0.153063  \n",
      "1                  -0.318145                 0.657003 -0.153063  \n",
      "2                  -0.318145                -1.522062 -0.153063  \n",
      "3                  -0.318145                 0.657003 -0.153063  \n",
      "4                  -0.318145                 0.657003 -0.153063  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"AI \"\"\"\n",
    "# Escalar el conjunto de prueba real utilizando el scaler ajustado\n",
    "xTestRealEscalado = scaler.transform(xTestReal)\n",
    "\n",
    "# Convertir a DataFrame para mantener los nombres de las columnas (opcional)\n",
    "xTestRealEscalado = pd.DataFrame(xTestRealEscalado, columns=xTestReal.columns)\n",
    "\n",
    "# Ver los datos escalados\n",
    "print(xTestRealEscalado.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "una vez teniendo los datos escalados, ya se puede hacer una predicción. Iba a escoger adaboost porque salió con mejor accuracy, pero al parecer no soporta el estilo neutron (no maneja NaNs de forma nativa), por lo que usaré random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = rf.predict(xTestRealEscalado)\n",
    "# AI lo de abajo\n",
    "final_predictions = pd.Series(final_predictions).map({0: False, 1: True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sin embargo, también tengo que adjuntar los IDs de los participantes. entonces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = pd.read_csv(\"test.csv\")\n",
    "# AI para ver como crear un df\n",
    "participantIDSTestSet = blah[\"PassengerId\"]\n",
    "final_predictions_for_csv = pd.DataFrame(\n",
    "    {\n",
    "        \"PassengerId\": participantIDSTestSet,\n",
    "        \"Transported\": final_predictions\n",
    "    }\n",
    ")\n",
    "final_predictions_for_csv.to_csv(\"Resultados.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4277, 2)\n"
     ]
    }
   ],
   "source": [
    "print(final_predictions_for_csv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las dimensiones son las esperadas. La precisión del modelo se encuentra en esta misma carpeta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
